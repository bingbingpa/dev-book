# 4장 카프카의 내부 동작 원리와 구현

### **4.1 카프카 리플리케이션**

- 리플리케이션 동작 개요
    - 카프카의 리플리케이션 동작을 위해 토픽 생성 시 필숫값으로 `replication factor` 라는 옵션을 설정해야 한다.
        - 카프카의 기본 도구인 kafka-topics.sh 명령어를 이용해 토픽을 생성

        ```bash
        ./bin/kafka-topics.sh --bootstrap-server '서버호스트' --create --topic '토픽명' --partitions 1 --replication-factor 3
        ```

        - 토픽 상세 보기 출력
            - **실제로 리플리케이션 되는 것은 토픽이 아니라 토픽을 구성하는 각각의 파티션들이다.**

            ```bash
            ./bin/kafka-topics.sh --bootstrap-server '서버호스트' --topic '토픽명' --describe
            ```

        - 세그먼트 파일 내용 확인
            - 카프카 클러스터를 이루는 다른 브로커들에 각각 접속해서 아래의 dump 명령어를 이용해 결과를 확인해보면 모든 브로커가 동일한 메시지를 갖고 있음을 알 수 있다.
            - 즉 프로듀서가 보낸 메시지 하나를 총 3대의 브로커들이 모두 갖고 있다.
            - 이렇게 카프카는 리플리케이션 팩터라는 옵션을 이용해 관리자가 지정한 수만큼의 리플리케이션을 가질 수 있기 때문에 장애가 발생해도 메시지 손실 없이 안정적으로 메시지를 주고 받을 수 있다.

            ```bash
            ./bin/kafka-dump-log.sh --print-data-log --files /data/kafka/logs/'토픽명'-'파티션번호'/'세그펀트 파일명'
            ```

- 리더와 팔로워
    - 카프카는 내부적으로 모두 동일한 리플리케이션들을 리더와 팔로워로 구분하고, 각자의 역할을 분담시킨다.
    - 리더는 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 그 리더를 통해서만 가능하다.
        - 다시 말해, 프로듀서는 모든 리플리케이션에 메시지를 보내는 것이 아니라 리더에게만 메시지를 전송한다.
        - 또한 컨슈머도 오직 리더로부터 메시지를 가져온다.
    - 팔로워들은 리더에 문제가 발생하거나 이슈가 있을 경우를 대비해 언제든지 새로운 리더가 될 준비를 한다.
        - 컨슈머가 토픽의 메시지를 꺼내 가는 것과 비슷한 동작으로 지속적으로 파티션의 리더가 새로운 메시지를 받았는지 확인하고, 새로운 메시지가 있다면 해당 메시지를 리더로부터 복제한다.
- 복제 유지와 커밋
    - 리더와 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여 있다.
        - 이렇게 리더와 팔로워를 별도의 그룹으로 나누는 이유는 기본적으로 해당 그룹 안에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있기 때문이다.
    - 리더는 읽고 쓰는 동작은 물록, 팔로워가 리플리케이션 동작을 잘 수행하고 있는지도 판단한다.
        - 만약 팔로워가 특정 주기의 시간만큼 복제 요청을 하지 않는다면, 리더는 해당 팔로워가 리플리케이션 동작에 문제가 발생했다고 판단해 ISR 그룹에서 추방한다.(새로운 리더가 될 자격 박탈)
    - ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게 된다.
        - 마지막 커밋 오프셋 위치는 하이워터마크(high water mark)라고 부른다.
        - 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있다.
        - **커밋되지 않은 메시지를 컨슈머가 읽을 수 있게 허용한다면 동일한 토픽의 파티션에서 컨슘했음에도 메시지가 일치하지 않는 현상이 발생할 수 있다.**
        - 모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 replication-offset-checkpoint 라는 파일에 마지막 커밋 오프셋 위치를 저장한다.
        - 만약 특정 토픽 또는 파티션에 복제가 되지 않거나 문제가 있다고 판단되는 경우, replication-offset-checkpoint 라는 파일의 내용을 확인하고 리플리케이션 되고 있는 다른 브로커들과 비교해 살펴보면, 어떤 브로커, 토픽, 파티션에 문제가 있는지를 파악할 수 있다.
- 리더와 팔로워의 단계별 리플리케이션 동작
    - **카프카에서는 리더와 팔로워 사이에서 ACK 를 주고 받는 통신이 없다.**(성능 향상)
    - **리더는 팔로워들이 보내는 리플리케이션 요청의 오프셋을 보고, 팔로워들이 어느 위치의 오프셋까지 리플리케이션을 성공했는지를 인지한다.**
    - 팔로워들로부터 다음 메시지에 대한 리플리케이션 요청을 받은 리더는 이전 메시지가 커밋되었다는 내용도 함께 전달한다.
    - 그럼 팔로워들은 리더와 동일하게 커밋을 표시한다.
    - 카프카에서 리더와 팔로워들의 리플리케이션 동작 방식은 리더가 푸쉬하는 방식이 아니라 팔로워들이 풀하는 방식으로 동작하는데, 풀 방식을 채택한 이유도 리플리케이션 동작에서 리더의 부하를 줄여주기 위해서이다.
- 리더에포크(LeaderEpoch)와 복구
    - 리더에포크는 카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지하기 위한 용도로 이용된다.
    - 리더에포크는 컨트롤러에 의해 관리되는 32비트의 숫자로 표현된다.
    - 리더에포크는 복구 동작 시 하이워터마크를 대체하는 수단으로도 활용된다.
        - 장애로 종료된 후 복구된 후에 리더에포크가 없는 경우는 자신의 하이워터마크보다 높은 메시지를 즉시 삭제한다.(메시지 손실)
        - 리더에포크를 사용하는 경우에는 하이워터마크보다 앞에 있는 메시지를 무조건 삭제하는 것이 아니라 리더에게 리더에포크 요청을 보낸다.
    - 리더에포크 상태는 kafka-logs/토픽-파티션경로/leader-epoch-checkpoint 파일에서 확인 가능

### **4.2 컨트롤러**

- 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게 되며, 파티션의 ISR 리스트 중에서 리더를 선출한다.
- 리더를 선출하기 위한 ISR 리스트 정보는 안전한 저장소에 보관되어 있어야 하는데, 가용성 보장을 위해 주키퍼에 저장된다.
- 컨트롤러는 브로커가 실패하는 것을 예의주시하고 있으며, 만약 브로커의 실패가 감지되면 즉시 ISR 리스트 중 하나를 새로운 파티션 리더로 선출한다.
- 그러고 나서 새로운 리더의 정보를 주키퍼에 기록하고, 변경된 정보를 모든 브로커에게 전달한다.
- 파티션의 리더가 다운됐다는 것은 해당 파티션의 리더가 없는 상태를 의미하며 카프카 클라이언트인 프로듀서나 컨슈머가 해당 파티션으로 읽기나 쓰기가 불가능하다.
- 하나의 파티션에 대해 리더 산출 작업이 약 0.2 초가 걸린다고 가정하면, 파티션이 하나라면 0.2초만에 완료된다.
    - 하지만 1만개의 파티션이라면... 전체 작업 소요시간은 약 2,000초가 걸린다.(3분)
    - 카프카 1.0.0 에서 6분 30초 소요되던 작업이, 불필요한 로깅을 없애고 주키퍼 비동기 API 가 반영된 카프카 버전 1.1.0 에서는 약 3초만에 완료
    - 책 시점으로 카프카 최신 버전 2.8.0이므로, 운영 환경에서 최신 버전을 사용하고 있다면 이런 문제에 대해 안심할 수 있다.
- 제어된 종료 상황에서는 종료되기 전, 컨트롤러가 해당 브로커가 리더로 할당된 전체 파티션에 대해 리더 선출 작업을 진행하기 때문에 다운 타임을 최소화 할 수 있다.
    - 제어된 종료를 사용하려면 controlled.shutdown.enable = true 설정이 server.properties 에 적용되어야 한다.
    - 브로커 설정은 kafka-config.sh 명령어를 이용하고 --broker 옵션과 확인하고자 하는 브로커 아이디를 명시해서 확인할 수 있다.

### **4.3 로그(로그 세그먼트)**

- 로그 세그먼트에는 메시지의 내용만 저장되는 것이 ㄴ아니라 메시지의 키, 밸류, 오프셋, 메시지 크기 같은 정보가 함께 저장되며, 로그 세그먼트 파일들은 브로커의 로컬 디스크에 보관된다.
- 로그 세그먼트는 기본 1GB 로 롤링된다.
- 로그 세그먼트 파일이 무한히 늘어날 경우를 대비해 로그 세그먼트에 대한 관리 계획을 수립해둬야 한다.
    - 로그 세그먼트 삭제
        - 이 옵션은 브로커의 설정 파일인 server.properties 에서 log.cleanup.policy 가 delete 로 명시되어야 한다.(디폴트가 delete)
        - 로그 세그먼트의 삭제 작업은 일정 주기를 가지고 체크하는데, 카프카의 기본값은 5분 주기이므로 5분 간격으로 로그 세그먼트 파일을 체크하면서 삭제 작업을 수행한다.
        - **카프카의 관리자는 토픽마다 보관 주기를 조정해서, 얼마만큼의 기간 동안 카프카에 로그를 저장할지를 결정하고 관리 할 수 있다.**
            - 관리자가 토픽에 별도의 `retention.ms` 옵션을 설정하지 않으면, 카프카의 server.properties 에 적용된 옵션값이 적용된다.(디폴트 7일)
            - `retention.bytes`라는 옵션을 이용해 지정된 크기를 기준으로도 로그 세그먼트를 삭제할 수 있다.
    - 로그 세그먼트 컴팩션
        - 컴팩션(compaction)은 카프카에서 제공하는 로그 세그먼트 관리 정책 중 하나로, 로그를 삭제하지 않고 컴팩션하여 보관할 수 있다.
        - 로그 컴팩션은 기본적으로 로컬 디스크에 저장되어 있는 세그먼트를 대상으로 실행되는데, 현재 활성화된 세그먼트는 제외하고 나머지 세그먼트들을 대상으로 컴팩션이 실행된다.
        - 카프카에서 로그 세그먼트를 컴팩션하면 메시지(레코드)의 키값을 기준으로 마지막의 데이터만 보관하게 된다.
        - **로그 컴팩션 기능을 사용하고자 한다면, 카프카로 메시지를 전송할 때 키도 필숫값으로 전송해야 한다.**(원래 키는 필숫값 아님)
        - 카프카에서 로그 컴팩션 작업이 실행되는 동안 브로커의 과도한 입출력 부하가 발생할 수 있으니 유의해야 한다.
        - 로그 컴팩션 관련 옵션
          ![](https://opensesame.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff9c6c1aa-c89b-416c-ad71-8aa4df98907c%2FUntitled.png?table=block&id=eeda5bf3-75a3-4590-9cc3-6ef4ae5d82e9&spaceId=f4775408-d01f-42fb-8eae-1614cb98b0ef&width=2000&userId=&cache=v2)